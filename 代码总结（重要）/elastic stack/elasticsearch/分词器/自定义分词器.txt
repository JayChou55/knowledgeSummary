通过自定义Character Filters（对原始文本进行过滤）、
Tokenizer（将原始文本按照一定规则切分为单词，自带如standard、letter按照非字符类分割、whitspace空格、UAX URL EMAIL、NGram和Edge NGram连词分割、Path Hierarchy按照文件路径分割）、
Token Filter（对于tokenizer输出的单词（term）进行增删修等操作，自带的如 lowercase，stop删除，NGram和Edge NGram ，Synonym添加近义词
自定义分词器测试
	POST _analyze
	{
	  "tokenizer": "standard", 
	  "filter": ["lowercase"], 
	  "text": "HELLO WORLD"
	}


	POST _analyze
	{
		"text": "a HELLO, WORLD!"
	  "tokenizer": "standard", 
	  "filter": ["lowercase",
		"stop",
		{
			"type":"ngram" #最小分割4，最大也是，每4个切一次，HELL ELLO LLOW，4-4是个范围
			"min_gram":4,
			"max_gram":4
			
		}
	  ]
	  
	}

自定义分词
	PUT test_index
	{
		"setting":{
			"analysis":{
				"char_filter":{},
				"tokenizer":{},
				"filter"{},
				"analyzer"{}
			}
		}
	}
示例
	PUT test_index
	{
	  "settings": {
		"analysis": {
		  "analyzer": {
			"my_custom_analyzer":{
			  "type":"custom",
			  "tokenizer":"standard",
			  "char_filter":[
				"html_strip"
				],
				"filter":[
					"lowercase",
					"asciifolding"
				  ]
			}
		  }
		}
	  
	  }
	}

	POST test_index/_analyze
	{
	  "analyzer": "my_custom_analyzer", 
	  "text":"<s>ASDASD sda'x"	  
	}
使用场景：
	创建或者更新文档时，会对相应文档进行分词
	查询时候
		查询时分词
		POST text_index/_search
		{
			"query":{
				"match":{
					"message":{
						"query":"hello",
						"analyzer":"standard"
					}
				}
			}
		}
	通过index mapping设置search_analyzer实现
	PUT test_index{
		"mappings":{
			"doc":{
				"properties":{
					"title":{
						"type":"text",
						"analyzer":"whitespace",
						"search_analyzer":"standard"
					}
				}
			}
		}
	}
建议：
	明确是否要分词，不用分词的字段将type设置keyword，节省空间和提高性能
	善用_analyze API,查看文档具体分词结果

全文搜索引擎会用某种算法对要建索引的文档进行分析， 从文档中提取出若干Token(词元)， 这些算法称为Tokenizer(分词器)；这些Token会被进一步处理， 比如转成小写等， 
这些处理算法被称为Token Filter(词元处理器), 被处理后的结果被称为Term(词)， 文档中包含了几个这样的Term被称为Frequency(词频)。 引擎会建立Term和原文档的Inverted Index(倒排索引)， 
这样就能根据Term很快到找到源文档了。 文本被Tokenizer处理前可能要做一些预处理， 比如去掉里面的HTML标记， 这些处理的算法被称为Character Filter(字符过滤器)， 
这整个的分析算法被称为Analyzer(分析器)。 
	ES内置了很多Analyzer, 还有很多第三方的Analyzer插件， 比如一些处理中文的Analyzer(中文分词)。 
analyzer、 tokenizer（将原始文本按照一定规则切分单词）、 tokenizer filter（针对tokenizer处理的单词在加工）可以在elasticsearch.yml 配置， 下面是配置例子
index :
    analysis :
        analyzer :
            standard :
                type : standard
                stopwords : [stop1, stop2]
            myAnalyzer1 :
                type : standard
                stopwords : [stop1, stop2, stop3]
                max_token_length : 500
            # configure a custom analyzer which is
            # exactly like the default standard analyzer
            myAnalyzer2 :
                tokenizer : standard
                filter : [standard, lowercase, stop]
        tokenizer :
            myTokenizer1 :
                type : standard
                max_token_length : 900
            myTokenizer2 :
                type : keyword
                buffer_size : 512
        filter :
            myTokenFilter1 :
                type : stop
                stopwords : [stop1, stop2, stop3, stop4]
            myTokenFilter2 :
                type : length
                min : 0
                max : 2000

analyzer：ES内置若干analyzer, 另外还可以用内置的character filter, tokenizer, token filter组装一个analyzer(custom analyzer)， 比如
index :
    analysis :
        analyzer :
            myAnalyzer :
                tokenizer : standard
                filter : [standard, lowercase, stop]

如果你要使用第三方的analyzer插件，需要先在配置文件elasticsearch.yml中注册, 下面是配置IkAnalyzer的例子
index:
  analysis:
    analyzer:      
      ik:
          alias: [ik_analyzer]
          type: org.elasticsearch.index.analysis.IkAnalyzerProvider

当一个analyzer在配置文件中被注册到一个名字(logical name)下后，在mapping定义或者一些API里就可以用这个名字来引用该analyzer了，比如
"message": {
    "type": "string",
    "indexAnalyzer": "ik",
    "searchAnalyzer": "ik"
}

如果没有指定索引和搜索用的analyzer，ES会用默认的analyzer来处理，也就是名字(logical name)为`default`, `default_index`, `default_search`的analyzer。 

从名字可以看出来，`default`是索引和搜索时用的默认的analyzer，`default_index`是索引时用的默认的analyzer， `default_search`是查询时用的默认analyzer。

下面是在elasticsearch.yml中配置默认analyzer的例子
index:

  analysis:

    analyzer:

        default_index:

            tokenizer: standard

            filter: [standard, lowercase, my_synonym, my_snow]

        default_search:

            tokenizer: standard

            filter: [standard, lowercase, stop]

ES内置的一些analyzer。
| analyzer              | logical name  | description                               |

| ----------------------|:-------------:| :-----------------------------------------|

| standard analyzer     | standard      | standard tokenizer, standard filter, lower case filter, stop filter |

| simple analyzer       | simple        | lower case tokenizer                      |

| stop analyzer         | stop          | lower case tokenizer, stop filter         |

| keyword analyzer      | keyword       | 不分词，内容整体作为一个token(not_analyzed) |

| pattern analyzer      | whitespace    | 正则表达式分词，默认匹配\W+                 |

| language analyzers    | [lang](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html)  | 各种语言 |

| snowball analyzer     | snowball      | standard tokenizer, standard filter, lower case filter, stop filter, snowball filter |

| custom analyzer       | custom        | 一个Tokenizer, 零个或多个Token Filter, 零个或多个Char Filter |

tokenizer：ES内置的tokenizer列表。
| tokenizer             | logical name  | description                           |

| ----------------------|:-------------:| :-------------------------------------|

| standard tokenizer    | standard      |                                       |

| edge ngram tokenizer  | edgeNGram     |                                       |

| keyword tokenizer     | keyword       | 不分词                                 |

| letter analyzer       | letter        | 按单词分                               |

| lowercase analyzer    | lowercase     | letter tokenizer, lower case filter   |

| ngram analyzers       | nGram         |                                       |

| whitespace analyzer   | whitespace    | 以空格为分隔符拆分                      |

| pattern analyzer      | pattern       | 定义分隔符的正则表达式                  |

| uax email url analyzer| uax_url_email | 不拆分url和email                       |

| path hierarchy analyzer| path_hierarchy| 处理类似`/path/to/somthing`样式的字符串|

token filter：ES内置的token filter列表。
| token filter          | logical name  | description                           |

| ----------------------|:-------------:| :-------------------------------------|

| standard filter       | standard      |                                       |

| ascii folding filter  | asciifolding  |                                       |

| length filter         | length        | 去掉太长或者太短的                      |

| lowercase filter      | lowercase     | 转成小写                               |

| ngram filter          | nGram         |                                       |

| edge ngram filter     | edgeNGram     |                                       |

| porter stem filter    | porterStem    | 波特词干算法                            |

| shingle filter        | shingle       | 定义分隔符的正则表达式                  |

| stop filter           | stop          | 移除 stop words                        |

| word delimiter filter | word_delimiter| 将一个单词再拆成子分词                   |

| stemmer token filter  | stemmer       |                                        |

| stemmer override filter| stemmer_override|                                     |

| keyword marker filter | keyword_marker|                                        |

| keyword repeat filter | keyword_repeat|                                        |

| kstem filter          | kstem         |                                        |

| snowball filter       | snowball      |                                        |

| phonetic filter       | phonetic      | [插件](https://github.com/elasticsearch/elasticsearch-analysis-phonetic) |

| synonym filter        | synonyms      | 处理同义词                              |

| compound word filter  | dictionary_decompounder, hyphenation_decompounder | 分解复合词  |

| reverse filter        | reverse       | 反转字符串                              |

| elision filter        | elision       | 去掉缩略语                              |

| truncate filter       | truncate      | 截断字符串                              |

| unique filter         | unique        |                                        |

| pattern capture filter| pattern_capture|                                       |

| pattern replace filte | pattern_replace| 用正则表达式替换                        |

| trim filter           | trim          | 去掉空格                                |

| limit token count filter| limit       | 限制token数量                           |

| hunspell filter       | hunspell      | 拼写检查                                |

| common grams filter   | common_grams  |                                        |

| normalization filter  | arabic_normalization, persian_normalization |          |

character filter：ES内置的character filter列表
| character filter          | logical name  | description               |

| --------------------------|:-------------:| :-------------------------|

| mapping char filter       | mapping       | 根据配置的映射关系替换字符   |

| html strip char filter    | html_strip    | 去掉HTML元素               |

| pattern replace char filter| pattern_replace| 用正则表达式处理字符串    |