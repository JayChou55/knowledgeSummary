一、solr查询参数使用说明
	q C 查询字符串，必须的。Solr 中用来搜索的查询。
		有关该语法的完整描述，请参阅 参考资料 中的 “Lucene QueryParser Syntax”。可以通过追加一个分号和已索引且未进行断词的字段的名称来包含排序信息。默认的排序是 score
		desc，指按记分降序排序。           q=myField:Java AND otherField:developerWorks; date asc此查询搜索指定的两个字段并根据一个日期字段对结果进行排序。
	start C 返回第一条记录在完整找到结果中的偏移位置，0开始，一般分页用。
	rows C 指定返回结果最多有多少条记录，配合start来实现分页。
	sort C 排序，格式：sort=<field name>+<desc|asc>[,<field name>+<desc|asc>]… 。
	示例：（inStock desc, price asc）表示先 “inStock” 降序, 再 “price” 升序，默认是相关性降序。
	wt C (writer type)指定输出格式，可以有 xml, json, php, phps, 后面 solr 1.3增加的，要用通知我们，因为默认没有打开。
	fq C （filter query）过虑查询，作用：在q查询符合结果中同时是fq查询符合的，
	fl- field作为逗号分隔的列表指定文档结果中应返回的 Field 集。默认为 “*”，指所有的字段。“score” 指还应返回记分。例如 *,score
	将返回所有字段及得分。用solrj的bean时，得在query中指定 query.set("fl", "*,score");
	q.op C 覆盖schema.xml的defaultOperator（有空格时用"AND"还是用"OR"操作逻辑），一般默认指定
	df C 默认的查询字段，一般默认指定
	qt C （query type）指定那个类型来处理查询请求，一般不用指定，默认是standard。
	indent C 返回的结果是否缩进，默认关闭，用 indent=true|on 开启，一般调试json,php,phps,ruby输出才有必要用这个参数。
	version C 查询语法的版本，建议不使用它，由服务器指定默认值。
	hight:
		 hl-highlight，h1=true，表示采用高亮。可以用h1.fl=field1,field2 来设定高亮显示的字段。
	hl.fl: 用空格或逗号隔开的字段列表。要启用某个字段的highlight功能，就得保证该字段在schema中是stored。如果该参数未被给出，那么就会高 亮默认字段 standard handler会用df参数，dismax字段用qf参数。你可以使用星号去方便的高亮所有字段。如果你使用了通配符，那么要考虑启用 hl.requiredFieldMatch选项。
	hl.requireFieldMatch:
	如果置为true，除非该字段的查询结果不为空才会被高亮。它的默认值是false，意味 着它可能匹配某个字段却高亮一个不同的字段。如果hl.fl使用了通配符，那么就要启用该参数。尽管如此，如果你的查询是all字段（可能是使用 copy-field 指令），那么还是把它设为false，这样搜索结果能表明哪个字段的查询文本未被找到
	hl.usePhraseHighlighter:
	如果一个查询中含有短语（引号框起来的）那么会保证一定要完全匹配短语的才会被高亮。
	hl.highlightMultiTerm
	如果使用通配符和模糊搜索，那么会确保与通配符匹配的term会高亮。默认为false，同时hl.usePhraseHighlighter要为true。
	hl.snippets：
	这是highlighted片段的最大数。默认值为1，也几乎不会修改。如果某个特定的字段的该值被置为0（如f.allText.hl.snippets=0），这就表明该字段被禁用高亮了。你可能在hl.fl=*时会这么用。
	hl.fragsize:
	每个snippet返回的最大字符数。默认是100.如果为0，那么该字段不会被fragmented且整个字段的值会被返回。大字段时不会这么做。
	hl.mergeContiguous:
	如果被置为true，当snippet重叠时会merge起来。
	hl.maxAnalyzedChars:
	会搜索高亮的最大字符，默认值为51200，如果你想禁用，设为-1
	hl.alternateField:
	如果没有生成snippet（没有terms 匹配），那么使用另一个字段值作为返回。
	hl.maxAlternateFieldLength:
	如果hl.alternateField启用，则有时需要制定alternateField的最大字符长度，默认0是即没有限制。所以合理的值是应该为
	hl.snippets * hl.fragsize这样返回结果的大小就能保持一致。
	hl.formatter:一个提供可替换的formatting算法的扩展点。默认值是simple，这是目前仅有的选项。显然这不够用，你可以看看org.apache.solr.highlight.HtmlFormatter.java 和 solrconfig.xml中highlighting元素是如何配置的。
	注意在不论原文中被高亮了什么值的情况下，如预先已存在的em tags，也不会被转义，所以在有时会导致假的高亮。
	hl.fragmenter:
	这个是solr制 定fragment算法的扩展点。gap是默认值。regex是另一种选项，这种选项指明highlight的边界由一个正则表达式确定。这是一种非典型 的高级选项。为了知道默认设置和fragmenters (and formatters)是如何配置的，可以看看solrconfig.xml中的highlight段。
	regex 的fragmenter有如下选项：
	hl.regex.pattern:正则表达式的pattern
	hl.regex.slop:这是hl.fragsize能变化以适应正则表达式的因子。默认值是0.6，意思是如果hl.fragsize=100那么fragment的大小会从40-160.
	这些值都可以在select中加入，也可以用solrj的api去设定，也可以配置在solrconfig.xml中配置。
示例如下：
<requestHandler name="search" class="solr.SearchHandler" default="true">
    <!C default values for query parameters can be specified, these
         will be overridden by parameters in the request
      C>
     <lst name="defaults">
       <str name="echoParams">explicit</str>
       <int name="rows">10</int>
       <bool name="hl">true</bool> 
       <str name="hl.fl">title,content</str>  
       <str name="f.content.hl.fragsize">200</str>
       <str name="mlt.qf">
         id^10.0 title^10.0 content^1.0
       </str>
     </lst>
</requestHandler>


//导入xml
//数据库导入solrconfig
	默认数据库导入不开启 dataimpor处理器要配置
	要配置对应jar包 2个solr的 一个数据苦的



















二. schema.xml文档注释中的信息
 
1、为了改进性能，可以采取以下几种措施：
将所有只用于搜索的，而不需要作为结果的field（特别是一些比较大的field）的stored设置为false
将不需要被用于搜索的，而只是作为结果返回的field的indexed设置为false
删除所有不必要的copyField声明
为了索引字段的最小化和搜索的效率，将所有的 text fields的index都设置成field，然后使用copyField将他们都复制到一个总的 text field上，然后对他进行搜索。
为了最大化搜索效率，使用java编写的客户端与solr交互（使用流通信）
在服务器端运行JVM（省去网络通信），使用尽可能高的Log输出等级，减少日志量。
2、schema名字
<schema name="example" version="1.2">
name：标识这个schema的名字
version：现在版本是1.2
3、filedType
<fieldTypename="string" class="solr.StrField" sortMissingLast="true" omitNorms="true" />
name：标识而已。
class和其他属性决定了这个fieldType的实际行为。（class以solr开始的，都是在org.appache.solr.analysis包下）
可选的属性：
sortMissingLast和sortMissingFirst两个属性是用在可以内在使用String排序的类型上（包括：string,boolean,sint,slong,sfloat,sdouble,pdate）。
sortMissingLast="true"，没有该field的数据排在有该field的数据之后，而不管请求时的排序规则。
sortMissingFirst="true"，跟上面倒过来呗。
2个值默认是设置成false
StrField类型不被分析，而是被逐字地索引/存储。
StrField和TextField都有一个可选的属性“compressThreshold”，保证压缩到不小于一个大小（单位：char）
<fieldType name="text" class="solr.TextField" positionIncrementGap="100">
solr.TextField 允许用户通过分析器来定制索引和查询，分析器包括 一个分词器（tokenizer）和多个过滤器（filter）
positionIncrementGap：可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误。
<tokenizerclass="solr.WhitespaceTokenizerFactory" />
空格分词，精确匹配。
<filterclass="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0" splitOnCaseChange="1" />
在分词和匹配时，考虑 "-"连字符，字母数字的界限，非字母数字字符，这样 "wifi"或"wi fi"都能匹配"Wi-Fi"。
<filterclass="solr.SynonymFilterFactory" synonyms="synonyms.txt" ignoreCase="true" expand="true" />
同义词
<filterclass="solr.StopFilterFactory" ignoreCase="true" words="stopwords.txt" enablePositionIncrements="true" />
在禁用字（stopword）删除后，在短语间增加间隔
stopword：即在建立索引过程中（建立索引和搜索）被忽略的词，比如is this等常用词。在conf/stopwords.txt维护。
4、fields
<fieldname="id" type="string" indexed="true" stored="true" required="true" />
name：标识而已。
type：先前定义的类型。
indexed：是否被用来建立索引（关系到搜索和排序）
stored：是否储存
compressed：[false]，是否使用gzip压缩（只有TextField和StrField可以压缩）
mutiValued：是否包含多个值
omitNorms：是否忽略掉Norm，可以节省内存空间，只有全文本field和need an index-time boost的field需要norm。（具体没看懂，注释里有矛盾）
termVectors：[false]，当设置true，会存储 term vector。当使用MoreLikeThis，用来作为相似词的field应该存储起来。
termPositions：存储 term vector中的地址信息，会消耗存储开销。
termOffsets：存储 term vector 的偏移量，会消耗存储开销。
default：如果没有属性需要修改，就可以用这个标识下。
<fieldname="text" type="text" indexed="true" stored="false" multiValued="true" />
包罗万象（有点夸张）的field，包含所有可搜索的text fields，通过copyField实现。
<copyFieldsource="cat" dest="text" />
<copyFieldsource="name" dest="text" />
<copyFieldsource="manu" dest="text" />
<copyFieldsource="features" dest="text" />
<copyFieldsource="includes" dest="text" />
在添加索引时，将所有被拷贝field（如cat）中的数据拷贝到text field中
作用：
将多个field的数据放在一起同时搜索，提供速度
将一个field的数据拷贝到另一个，可以用2种不同的方式来建立索引。
<dynamicFieldname="*_i" type="int" indexed="true" stored="true" />
如果一个field的名字没有匹配到，那么就会用动态field试图匹配定义的各种模式。
"*"只能出现在模式的最前和最后
较长的模式会被先去做匹配
如果2个模式同时匹配上，最先定义的优先
<dynamicFieldname="*" type="ignored" multiValued="true" />
如果通过上面的匹配都没找到，可以定义这个，然后定义个type，当String处理。（一般不会发生）
但若不定义，找不到匹配会报错。
5、其他一些标签
<uniqueKey>id</uniqueKey>
文档的唯一标识， 必须填写这个field（除非该field被标记required="false"），否则solr建立索引报错。
<defaultSearchField>text</defaultSearchField>
如果搜索参数中没有指定具体的field，那么这是默认的域。
<solrQueryParserdefaultOperator="OR" />
配置搜索参数短语间的逻辑，可以是"AND|OR"。
三、solrconfig.xml
1、索引配置
mainIndex 标记段定义了控制Solr索引处理的一些因素.
useCompoundFile：通过将很多 Lucene 内部文件整合到单一一个文件来减少使用中的文件的数量。这可有助于减少 Solr 使用的文件句柄数目，代价是降低了性能。除非是应用程序用完了文件句柄，否则false 的默认值应该就已经足够。
useCompoundFile：通过将很多Lucene内部文件整合到一个文件，来减少使用中的文件的数量。这可有助于减少Solr使用的文件句柄的数目，代价是降低了性能。除非是应用程序用完了文件句柄，否则false的默认值应该就已经足够了。
mergeFacor：决定Lucene段被合并的频率。较小的值（最小为2）使用的内存较少但导致的索引时间也更慢。较大的值可使索引时间变快但会牺牲较多的内存。（典型的 时间与空间 的平衡配置）
maxBufferedDocs：在合并内存中文档和创建新段之前，定义所需索引的最小文档数。段 是用来存储索引信息的Lucene文件。较大的值可使索引时间变快但会牺牲较多内存。
maxMergeDocs：控制可由Solr合并的 Document 的最大数。较小的值（<10,000）最适合于具有大量更新的应用程序。
maxFieldLength：对于给定的Document，控制可添加到Field的最大条目数，进而阶段该文档。如果文档可能会很大，就需要增加这个数值。然后，若将这个值设置得过高会导致内存不足错误。
unlockOnStartup：告知Solr忽略在多线程环境中用来保护索引的锁定机制。在某些情况下，索引可能会由于不正确的关机或其他错误而一直处于锁定，这就妨碍了添加和更新。将其设置为true可以禁用启动索引，进而允许进行添加和更新。（锁机制）
2、查询处理配置
query标记段中以下一些与缓存无关的特性：
maxBooleanClauses：定义可组合在一起形成以个查询的字句数量的上限。正常情况1024已经足够。如果应用程序大量使用了通配符或范围查询，增加这个限制将能避免当值超出时，抛出TooMangClausesException。
enableLazyFieldLoading：如果应用程序只会检索Document上少数几个Field，那么可以将这个属性设置为true。懒散加载的一个常见场景大都发生在应用程序返回一些列搜索结果的时候，用户常常会单击其中的一个来查看存储在此索引中的原始文档。初始的现实常常只需要现实很短的一段信息。若是检索大型的Document，除非必需，否则就应该避免加载整个文档。
query部分负责定义与在Solr中发生的时间相关的几个选项：
概念：Solr（实际上是Lucene）使用称为Searcher的Java类来处理Query实例。Searcher将索引内容相关的数据加载到内存中。根据索引、CPU已经可用内存的大小，这个过程可能需要较长的一段时间。要改进这一设计和显著提高性能，Solr引入了一张“温暖”策略，即把这些新的Searcher联机以便为现场用户提供查询服务之前，先对它们进行“热身”。
newSearcher和firstSearcher事件，可以使用这些事件来制定实例化新Searcher或第一个Searcher时，应该执行哪些查询。如果应用程序期望请求某些特定的查询，那么在创建新Searcher或第一个Searcher时就应该反注释这些部分并执行适当的查询。
query中的智能缓存：
filterCache：通过存储一个匹配给定查询的文档 id 的无序集，过滤器让 Solr 能够有效提高查询的性能。缓存这些过滤器意味着对Solr的重复调用可以导致结果集的快速查找。更常见的场景是缓存一个过滤器，然后再发起后续的精炼查询，这种查询能使用过滤器来限制要搜索的文档数。
queryResultCache：为查询、排序条件和所请求文档的数量缓存文档 id 的有序集合。
documentCache：缓存Lucene Document，使用内部Lucene文档id（以便不与Solr唯一id相混淆）。由于Lucene的内部Document id 可以因索引操作而更改，这种缓存不能自热。
Named caches：命名缓存是用户定义的缓存，可被 Solr定制插件 所使用。
其中filterCache、queryResultCache、Named caches（如果实现了org.apache.solr.search.CacheRegenerator）可以自热。
每个缓存声明都接受最多四个属性：
class：是缓存实现的Java名
size：是最大的条目数
initialSize：是缓存的初始大小
autoWarmCount：是取自旧缓存以预热新缓存的条目数。如果条目很多，就意味着缓存的hit会更多，只不过需要花更长的预热时间。
对于所有缓存模式而言，在设置缓存参数时，都有必要在内存、cpu和磁盘访问之间进行均衡。统计信息管理页（管理员界面的Statistics）对于分析缓存的 hit-to-miss 比例以及微调缓存大小的统计数据都非常有用。而且，并非所有应用程序都会从缓存受益。实际上，一些应用程序反而会由于需要将某个永远也用不到的条目存储在缓存中这一额外步骤而受到影响。